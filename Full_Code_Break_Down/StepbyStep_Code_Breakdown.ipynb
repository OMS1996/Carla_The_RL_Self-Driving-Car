{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carla The Reinforcement Learning Self-Driving Car (Version Morra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<p>In this notebook we will endeavor to build a Self-Driving car using a reinforcement learning approach in an environment called the Carla Simulator based on RGB data and Collision data.\n",
    "</p>\n",
    "\n",
    "<p>Carla is an environment built using unreal engine and C++ to support development and simulation of autonomous driving that uses the OpenDRIVE standard (1.4 as today) to define roads and urban settings.Where the environment is the server whether it is a local host or a remote host and the agent (In this case the car) is the client. for more information please the following links:\n",
    "</p>\n",
    "\n",
    "- <a href=\"http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf\">CARLA: An Open Urban Driving Simulator</a>\n",
    "\n",
    "- <a href=\"http://carla.org/\">The official website for the carla open-source simulator for autonmous driving research.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements & Installation\n",
    "For requirements and installation please review the following links here:\n",
    "- <a href=\"https://carla.readthedocs.io/en/latest/start_quickstart/#requirements\">Requirements</a>\n",
    "- <a href=\"https://carla.readthedocs.io/en/latest/start_quickstart/#installation-summary\">How to install carla</a>\n",
    "\n",
    "- Install Tensorflow version (2.3.1) and Keras version (2.4.3) otherwise you may need to update or downgrade some of the commands, hence it is advisable to use a virtual environment.\n",
    "\n",
    "- <a href=\"https://github.com/carla-simulator/carla/blob/master/Docs/download.md\">Don't forget to Download relatively recenet version of the carla simulatior</a>\n",
    "----------------------------------------------------\n",
    "Once everything is setup you must ensure that you have that you have `CarlaUE4.exe` running. or if you are on linux run the command `./CarlaUE4.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://carla.readthedocs.io/en/latest/img/welcome.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://carla.readthedocs.io/en/latest/img/welcome.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carla Python API\n",
    "Carla has a <a href=\"https://carla.readthedocs.io/en/latest/python_api/\">Python API</a> that facilitates the process of building intelligent systems in python. However there are concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://carla.readthedocs.io/en/latest/img/carla_modules.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://carla.readthedocs.io/en/latest/img/carla_modules.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "- 1st- World and client.\n",
    "- 2nd- Actors and blueprints.\n",
    "- 3rd- Maps and navigation.\n",
    "- 4th- Sensors and data.\n",
    "\n",
    "All these concepts that were built using OOP. So most of these core concepts were treated as objects and classes. for example:\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "        # starting the environement\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(3.0)\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Overview.\n",
    "The code will be broken down into four parts.\n",
    "- Global variables\n",
    "- Two classes\n",
    "    - EnvControl: Is a class that handles the carla environment using the Python API for Carla, processes the images ....etc\n",
    "    - Deep_Cue_Network_Agent: This is the class where the actual DQN algorithm is written in conjunction to the VGG-16 Network. \n",
    "- The main function: This is where everything comes together and the results are being recorded episodically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The necassary libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, GlobalMaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "# Getting the necassary files through glob\n",
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "# Importing the carla API Classes.\n",
    "import carla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Global variables.\n",
    "<p>The Global variables here are variables that will be used everywhere in the script. Some of the properties of these global variables is that, they can affect the behavior of the entire script. so it would be better to put them aside in the event of wanting to fine tune them.</p>\n",
    "\n",
    "The global variables handle:\n",
    "- Total script run time calculation.\n",
    "- window size for the image data.\n",
    "- the size of the minibatch and the training batch\n",
    "- the name of the model, for recording purposes.\n",
    "- the minumum possible reward in the event that an anomly would occur.\n",
    "- DQN hyperparameters.\n",
    "    - Episodes\n",
    "    - Discount rate\n",
    "    - Epsilon\n",
    "    - Epsilon decay.\n",
    "    - Minimum epsilon.\n",
    "- recording and updating variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL VARIABLES\n",
    "very_start = time.time()\n",
    "\n",
    "# The frame dimesions.\n",
    "IM_WIDTH = 500\n",
    "IM_HEIGHT = 500\n",
    "\n",
    "# The minibatch size\n",
    "minibatch_size = 16\n",
    "\n",
    "# Predictions size\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "\n",
    "# The size of the training batch.\n",
    "TRAINING_BATCH_SIZE = minibatch_size // 4\n",
    "\n",
    "# CNN archietecture\n",
    "MODEL_NAME = \"VGG16_GlobalMax2DPool\"\n",
    "\n",
    "# Lowest possible reward\n",
    "MIN_REWARD = - 100 \n",
    "\n",
    "# The number of episodes (NOT STEPS)\n",
    "EPISODES = 500\n",
    "\n",
    "# The discount rate from the bellman equation\n",
    "DISCOUNT = 0.997\n",
    "\n",
    "# Whether or not we will use the network\n",
    "# The probability of using the network increases over time\n",
    "# However it won't go below 0.001\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.997\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "# Get rewards every\n",
    "GET_REWARD_STATS_EVERY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnvControl\n",
    "The `EnvControl` class controls the environment. It manages a plethora of different aspects of the environment and acts as an umpire in regards to the relationship between the car and the environment.\n",
    "\n",
    "------------------\n",
    "#### Some of the things, the class handles:\n",
    "- Server/Client relations (the car and the environment).\n",
    "- Spawning a car of model type tesla (The agent) at random location in carla environment [Every time it restarts].\n",
    "- Provides the means for manual control.\n",
    "- Attaching the camera to the front of the car and preproccessing the images it recieves into `RGB` format.\n",
    "- One step dynamics: Meaning it defines how the car should take a step in its environment through a governing body of rules.\n",
    "- Collision detection.\n",
    "- Destroys actors at the end of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvControl:\n",
    " \n",
    "    \n",
    "    # Full steering amount\n",
    "    STEER_AMT = 0.7\n",
    "    \n",
    "    # The size of the frame.\n",
    "    im_width = 500\n",
    "    im_height = 500\n",
    "    \n",
    "    # The values coming in from the front camera after being processed.\n",
    "    front_camera = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # starting the environement\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(3.0)\n",
    "        \n",
    "        # Getting the world into a variable.\n",
    "        # Getting the car from the blueprint object\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "        # The duration of the episode\n",
    "        self.SECONDS_PER_EPISODE = 10\n",
    "    \n",
    "    def RESTART(self):\n",
    "        # recording collisions\n",
    "        self.collision_hist = []\n",
    "        # Recording the list of actors for later destruction\n",
    "        self.actor_list = []   \n",
    "        \n",
    "        # Getting the spawn locations (there are 200 spawn locations)\n",
    "        # Then creating the vehicle \n",
    "        # Finally storing it into the actor list for later destruction\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "        \n",
    "        # Finding the RGB senor blueprint.\n",
    "        # Initializing it with necassary parameters.\n",
    "        # With front view\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "        \n",
    "        ## Creating the first Sensor (Sensor No. 1)\n",
    "        # Getting the car location\n",
    "        # relative to the car location putting the camera in the front area\n",
    "        # Attaching it and assigning it\n",
    "        # Putting it into the actor list\n",
    "        # All the data coming into that sensor, get the RGB version of it.\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Applying control as stationary\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        time.sleep(4)\n",
    "        \n",
    "        ## Creating the second sensor (Sensor No. 2)\n",
    "        # Getting the blueprint for it.\n",
    "        # Putting into the actors list for later destruction.\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        \n",
    "        # Be stand by till you recieve input  -----\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        # getting the starting time of the episode.\n",
    "        self.episode_start = time.time()\n",
    "        \n",
    "        # Putting the car in a stationary position\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        \n",
    "        return self.front_camera # Returning what the sensor sees.\n",
    "    \n",
    "    def process_img(self, image):\n",
    "        raw = np.array(image.raw_data) # convert to an array\n",
    "        reshaped_image = raw.reshape((self.im_height, self.im_width, 4)) # was flattened, so we're going to shape it.\n",
    "        rgb_image = reshaped_image[:, :, :3] # remove the alpha\n",
    "        cv2.imshow(\"\", rgb_image)\n",
    "        cv2.waitKey(1)\n",
    "        self.front_camera = rgb_image\n",
    "    \n",
    "    # Book keeping\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        - ALL actions right,left,forward\n",
    "        - handle for the observation, possible collision, and reward\n",
    "        \n",
    "        '''\n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle = 0.7, steer = - 1 * self.STEER_AMT)) # left\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle = 0.5, steer = 0.0)) # Half-throttle\n",
    "        elif action == 2:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle = 1.0, steer= 0)) # Full-throttle\n",
    "        elif action == 3:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle = 0.7, steer = 1 * self.STEER_AMT)) # right\n",
    "            \n",
    "        \n",
    "        # Getting the velocity\n",
    "        v = self.vehicle.get_velocity()\n",
    "        \n",
    "        # Getting the resultant velocity meter/sec\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "        \n",
    "        ## The Reward Shaping\n",
    "        # if there is a collision Then just end the episode\n",
    "        #  If there isnt a collision check the speed if all is well reward it\n",
    "        if len(self.collision_hist) != 0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        elif kmh < 35:\n",
    "            done = False\n",
    "            reward = -15\n",
    "        elif kmh > 120:\n",
    "            done = False\n",
    "            reward = - 10\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 15\n",
    "        # Is the episode over already ?\n",
    "        if self.episode_start + self.SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "        # Getting the new input based, the reward and the terminal state boolean\n",
    "        return self.front_camera, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep_Cue_Network_Agent class\n",
    "This class consists of six different essential methods.\n",
    "\n",
    "- `__init__` : Instantiates a Deep_Cue_Network_Agent Object\n",
    "\n",
    "- `build_model`: This method creates globally pooled version of the VGG-16 architecture.\n",
    "\n",
    "- `update_replay_memory`: Updates the replay memory\n",
    "\n",
    "- `train` : the train method updates the q-values for the DQN algorithm.\n",
    "\n",
    "- `predict_qs`: gets the q-values using the neural network's .predict() method after reshaping and processing the input first.\n",
    "\n",
    "- `thread_loop`: creates an infinite loop that acts as a thread for the purposes of aiding the training process, such that the agent would be predicting and fitting simultaneously and it can only be stopped through a termination flag that is triggered by the end of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Cue_Network_Agent:\n",
    "    def __init__(self):\n",
    "        # Target model and fitment model.\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # The size of the replay memory.\n",
    "        self.REPLAY_MEMORY_SIZE = 4_000\n",
    "        \n",
    "        # Q - Learning specific variable\n",
    "        # The replay memory, using a deque (A queue that can be used from both sides).\n",
    "        self.replay_memory = deque(maxlen= self.REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # The minimum replay memory size.\n",
    "        self.MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "        \n",
    "        # State and network attributes\n",
    "            # Tracker\n",
    "            # termination boolean\n",
    "            # Last episode that was logged in\n",
    "            # Training flag\n",
    "        self.target_update_counter = 0 # Update tracker\n",
    "        self.terminate = False  # is it terminal state\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "        \n",
    "        # Update the target model every 5\n",
    "        self.UPDATE_TARGET_EVERY = 5\n",
    "        \n",
    "    def build_model(self):\n",
    "        '''\n",
    "        Handles building the model, using maxpooled version\n",
    "        of the VGG-16 archeticutre without the weights.\n",
    "        input :: None\n",
    "        output :: keras.model\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # Creating the VGG16\n",
    "        # Try with pre-trained network -----\n",
    "        base_model = VGG16(include_top = False, weights = None, input_shape= (IM_HEIGHT,IM_WIDTH ,3))\n",
    "        \n",
    "        # getting the output \n",
    "        x = base_model.output\n",
    "        \n",
    "        # the pooling method\n",
    "        x = GlobalMaxPool2D()(x)\n",
    "        \n",
    "        # Getting the predictions from the dense layer.\n",
    "        predictions = Dense(3, activation=\"linear\")(x)\n",
    "        \n",
    "        # setting the stage.\n",
    "        model = Model(inputs = base_model.input, outputs=predictions)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr = 0.009), metrics=[\"accuracy\"])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        '''\n",
    "         Funtion updates replay memory with the new frames.\n",
    "         Input: transition -> Tuple :: (current_state, action, reward, new_state, done)\n",
    "         Output: None\n",
    "        '''\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        if len(self.replay_memory) < self.MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        print(\"Training started !!!!\")\n",
    "        \n",
    "        # Getting a random sample from the replay memory relative to the minibatch size assigned.\n",
    "        minibatch = random.sample(self.replay_memory, minibatch_size)\n",
    "        \n",
    "        # CURRENT Q - VALUES\n",
    "        # Getting the current states from tuple which is at the 0th index\n",
    "        # normalizing the frame and storing it in current states\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        \n",
    "        # Using them to predict\n",
    "        current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "            \n",
    "        # New Q - values\n",
    "        # Getting the new state which is at the 3rd index\n",
    "        # Normalize the frames\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        # FUTURE Q_LIST\n",
    "        future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        # Change it into a Supervised learning problem, In somesense\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # Looping through the minibatch\n",
    "        # Updating the q value if it is not over\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                # The equation for updating the q-value\n",
    "                new_q = reward + DISCOUNT * np.max(future_qs_list[index])\n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            # Update the current \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # X and Y like supervised learning.\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        # preparing the tensorboard\n",
    "        log_this_step = False\n",
    "        if current_episode_ptr > self.last_logged_episode:\n",
    "            log_this_step = True\n",
    "        self.last_log_episode = current_episode_ptr\n",
    "        \n",
    "        # Fitting the model\n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size = TRAINING_BATCH_SIZE, verbose = 0, shuffle = False)\n",
    "\n",
    "        if log_this_step:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > self.UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "            \n",
    "    def predict_qs(self, state): ## predict_qs for only one\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    def thread_loop(self):\n",
    "        # toy data to warm up the model.\n",
    "        X = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, 3)).astype(np.float32)\n",
    "        \n",
    "        # Fitting the model\n",
    "        self.model.fit(X,y, verbose = False, batch_size = 1) # Fitting the model of batch size one.\n",
    "            \n",
    "        # Initisalization setup\n",
    "        self.training_initialized = True            \n",
    "            \n",
    "        # Infinite loop.\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function\n",
    "<p>Through the main function we want to utilize our GPU for training purposes and we are going to use 2048 MB The GPU memory, feel free to change that depending on your hardware. We create a folder to store the results and also a directory for storing the weights for the our neural network. after initializing the `EnvControl object` and the `Deep_Cue_Network_Agent`. we open up the thread so that we can predict at the same time. Then we run the agent for specified number of episodes in this case for 500 episodes.</p>\n",
    "\n",
    "One thing that is important to note is this block of code:\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                if np.random.random() > epsilon:\n",
    "                    # Get action from Q table [Use the newtwork, use the .predict()]\n",
    "                    action = np.argmax(agent.predict_qs(current_state))\n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, 4)\n",
    "                    # This takes no time, so we add a delay matching 10 FPS (prediction above takes longer)\n",
    "                    time.sleep(1/10)\n",
    "The reason for that is because this block of code handles the exploitation versus-the exploration in particular this line\n",
    "`if np.random.random() > epsilon:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # For stats\n",
    "    ep_rewards = []\n",
    "    ep_rewards.append(MIN_REWARD) # Starting out with a low reward\n",
    "    full_stat = []\n",
    "    \n",
    "    # The head\n",
    "    full_stat.append(['average_reward','Min_reward', 'max_reward', 'epsilon'])\n",
    "\n",
    "    # For more repetitive results\n",
    "    random.seed(40)\n",
    "    np.random.seed(40)\n",
    "    tf.random.set_seed(40) #https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/set_seed\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit= 1024*2)])\n",
    "      except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Create models folder\n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # Create agent and environment\n",
    "    agent = Deep_Cue_Network_Agent()\n",
    "    env = EnvControl()           \n",
    "    \n",
    "    # Start training thread and wait for training to be initialized\n",
    "    trainer_thread = Thread(target=agent.thread_loop, daemon=True)\n",
    "    trainer_thread.start()\n",
    "    while not agent.training_initialized:\n",
    "        time.sleep(0.01)\n",
    "            \n",
    "    # Initialize predictions - forst prediction takes longer as of initialization that has to be done\n",
    "    # It's better to do a first prediction then before we start iterating over episode steps\n",
    "    agent.predict_qs(np.ones((env.im_height, env.im_width, 3)))\n",
    "\n",
    "    ID = 0\n",
    "\n",
    "    # Iterate over episodes\n",
    "    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "            env.collision_hist = []\n",
    "\n",
    "            # Update tensorboard step every episode\n",
    "            current_episode_ptr = episode\n",
    "\n",
    "            # Restarting episode - RESTART episode reward and step number\n",
    "            episode_reward = 0\n",
    "            step = 1\n",
    "            \n",
    "            # RESTART environment and get initial state\n",
    "            current_state = env.RESTART()\n",
    "            \n",
    "            # RESTART flag and start iterating until episode ends\n",
    "            done = False\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            # the data frame\n",
    "            # Create the pandas DataFrame \n",
    "            df = pd.DataFrame() \n",
    "            df['Statistics'] = [\"average_reward\",\"min_reward\",\"max_reward\",\"epsilon\"]\n",
    "\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                if np.random.random() > epsilon:\n",
    "                    # Get action from Q table [Use the newtwork, use the .predict()]\n",
    "                    action = np.argmax(agent.predict_qs(current_state))\n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, 4)\n",
    "                    # This takes no time, so we add a delay matching 10 FPS (prediction above takes longer)\n",
    "                    time.sleep(1/10)\n",
    "                    \n",
    "                # Take a step within the episode.\n",
    "                # get all the information the new state and reward acquired and whether or not it is done.\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Total reward per episode.\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Update replay memory every step. \n",
    "                agent.update_replay_memory((current_state, action, reward, new_state, done))                   \n",
    "                \n",
    "                # Update the current state.\n",
    "                current_state = new_state\n",
    "                \n",
    "                # Update the step, register a new step.\n",
    "                step += 1\n",
    "                \n",
    "                # whether or not the episode is done.\n",
    "                if done:\n",
    "                    break       \n",
    "                \n",
    "            # Destroy all the actors objects at the end of the episode.\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()      \n",
    "                \n",
    "            # Time to get the metadata, the statistics basically.\n",
    "            # Append episode reward to a list and log stats (every given number of episodes)\n",
    "            \n",
    "            ep_rewards.append(episode_reward) # adding the current episode reward\n",
    "            \n",
    "            # Registiering the stats every certain amount of `episodes`, so we can keep track of it\n",
    "            # We are getting the average reward, the minumum and the maxiumum for the last certain amount of `episodes`\n",
    "            if not episode % GET_REWARD_STATS_EVERY or episode == 1:\n",
    "                \n",
    "                # Getting the stats\n",
    "                average_reward = sum(ep_rewards[-GET_REWARD_STATS_EVERY:])/len(ep_rewards[-GET_REWARD_STATS_EVERY:])\n",
    "                min_reward = min(ep_rewards[-GET_REWARD_STATS_EVERY:])\n",
    "                max_reward = max(ep_rewards[-GET_REWARD_STATS_EVERY:])\n",
    "                \n",
    "                # Making use of this update.\n",
    "                agent_stats = [average_reward,min_reward, max_reward, epsilon]\n",
    "                \n",
    "                print()\n",
    "                print(\"---------------------------\")\n",
    "                print(\"episode\",episode)\n",
    "                print(\"average_reward:min_reward:max_reward:epsilon\")\n",
    "                # Print stats.\n",
    "                print(agent_stats)\n",
    "                print(\"---------------------------\")\n",
    "                print()\n",
    "                full_stat.append(agent_stats)\n",
    "                \n",
    "                # Move on to the next.\n",
    "                \n",
    "\n",
    "                # Save model, but only when min reward is greater or equal a set value\n",
    "                if min_reward >= MIN_REWARD:\n",
    "                    agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>10.2f}max_{average_reward:_>10.2f}avg_{min_reward:_>7.2f}min_{EPISODES:_>4.0f}episodes__{int(time.time())}.model')                 \n",
    "                    \n",
    "            # Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon) \n",
    "\n",
    "    # Now that all the madness above is over let's end it it              \n",
    "    # Set termination flag for training thread\n",
    "    # Kill the thread\n",
    "    # Save the final model with its timestamp next to it,\n",
    "    agent.terminate = True\n",
    "    trainer_thread.join() \n",
    "    agent.model.save(f'models/{MODEL_NAME}__{int(time.time())}.model')  \n",
    "    \n",
    "    # Time at the end of the script\n",
    "    very_end = time.time()\n",
    "    \n",
    "    # The total time needed for the script to run\n",
    "    total_time_from_start_to_finish = very_end - very_start\n",
    "    \n",
    "    from pprint import pprint\n",
    "    \n",
    "    print()\n",
    "    pprint(full_stat)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    total_time_from_start_to_finish = total_time_from_start_to_finish /(60*60)\n",
    "    print(\"Time for the whole script to run is : \",total_time_from_start_to_finish)\n",
    "    \n",
    "    # Transfer the list of lists to pandas dataframe.\n",
    "    df = pd.DataFrame(full_stat[1:], columns=full_stat[0])\n",
    "    \n",
    "    # Save the df\n",
    "    df.to_csv(f'Carla_Metrics_AccEpsilon__{int(time.time())}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: All the images being used belong to the CARLA simulator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
